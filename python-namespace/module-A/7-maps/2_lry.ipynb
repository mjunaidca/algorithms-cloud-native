{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive into the concept of **Least Recently Used (LRU)** caching and how it relates to Maps. LRU is a common caching strategy that uses a combination of Maps (often HashMaps or dictionaries) and linked data structures to efficiently manage cached items.\n",
    "\n",
    "### 1. **What is LRU (Least Recently Used) Caching?**\n",
    "\n",
    "LRU caching is a technique used to keep track of a set of items, ensuring that the most recently accessed items remain in the cache, while the least recently accessed items are evicted when the cache reaches its capacity.\n",
    "\n",
    "### 2. **How LRU Caching Works**\n",
    "\n",
    "- **Cache Capacity**: The cache has a fixed size (capacity). When the cache is full and a new item needs to be added, the least recently used item is removed to make space for the new item.\n",
    "\n",
    "- **Access Order**: Each time an item is accessed (whether it's a read or write), it is moved to the most recent position in the cache.\n",
    "\n",
    "### 3. **LRU Cache Implementation Using Maps**\n",
    "\n",
    "To implement an LRU Cache, we need two main components:\n",
    "\n",
    "1. **A Map (HashMap or Dictionary)**: This is used to store the key-value pairs for O(1) average time complexity lookups.\n",
    "2. **A Doubly Linked List**: This helps in efficiently maintaining the order of access. The most recently used item is at the head, and the least recently used item is at the tail.\n",
    "\n",
    "### 4. **Basic Operations**\n",
    "\n",
    "- **Get(key)**: If the key exists in the cache, return the value and move the key to the head of the linked list to mark it as recently used.\n",
    "- **Put(key, value)**: Insert a new key-value pair into the cache. If the key already exists, update the value and move the key to the head. If the cache is at capacity, remove the item from the tail before inserting the new item.\n",
    "\n",
    "### 5. **Python Implementation of LRU Cache**\n",
    "\n",
    "Here is a basic implementation of an LRU Cache in Python using a combination of a dictionary and a doubly linked list:\n",
    "\n",
    "```python\n",
    "class Node:\n",
    "    def __init__(self, key, value):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.prev = None\n",
    "        self.next = None\n",
    "\n",
    "class LRUCache:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.cache = {}  # Map to store key -> Node\n",
    "        self.head = Node(0, 0)  # Dummy head\n",
    "        self.tail = Node(0, 0)  # Dummy tail\n",
    "        self.head.next = self.tail\n",
    "        self.tail.prev = self.head\n",
    "\n",
    "    def _remove(self, node):\n",
    "        prev = node.prev\n",
    "        next = node.next\n",
    "        prev.next = next\n",
    "        next.prev = prev\n",
    "\n",
    "    def _add(self, node):\n",
    "        prev = self.head\n",
    "        next = self.head.next\n",
    "        prev.next = node\n",
    "        node.prev = prev\n",
    "        node.next = next\n",
    "        next.prev = node\n",
    "\n",
    "    def get(self, key: int) -> int:\n",
    "        if key in self.cache:\n",
    "            node = self.cache[key]\n",
    "            self._remove(node)\n",
    "            self._add(node)\n",
    "            return node.value\n",
    "        return -1\n",
    "\n",
    "    def put(self, key: int, value: int) -> None:\n",
    "        if key in self.cache:\n",
    "            self._remove(self.cache[key])\n",
    "        node = Node(key, value)\n",
    "        self._add(node)\n",
    "        self.cache[key] = node\n",
    "        if len(self.cache) > self.capacity:\n",
    "            # Remove from the linked list and delete the LRU from the cache\n",
    "            lru = self.tail.prev\n",
    "            self._remove(lru)\n",
    "            del self.cache[lru.key]\n",
    "\n",
    "# Example Usage\n",
    "lru_cache = LRUCache(2)\n",
    "\n",
    "lru_cache.put(1, 1)\n",
    "lru_cache.put(2, 2)\n",
    "print(lru_cache.get(1))    # returns 1\n",
    "lru_cache.put(3, 3)        # evicts key 2\n",
    "print(lru_cache.get(2))    # returns -1 (not found)\n",
    "lru_cache.put(4, 4)        # evicts key 1\n",
    "print(lru_cache.get(1))    # returns -1 (not found)\n",
    "print(lru_cache.get(3))    # returns 3\n",
    "print(lru_cache.get(4))    # returns 4\n",
    "```\n",
    "\n",
    "### 6. **Explanation of the Implementation**\n",
    "\n",
    "- **Nodes**: Each key-value pair in the cache is stored in a `Node` object. The nodes are doubly linked, meaning each node has pointers to both its previous and next nodes.\n",
    "  \n",
    "- **Linked List**: The linked list maintains the order of usage. The head is the most recently used, and the tail is the least recently used.\n",
    "\n",
    "- **Get Operation**: The `get` method checks if the key is in the cache. If it is, it returns the value and moves the node to the front of the list to mark it as recently used. If the key isn't found, it returns -1.\n",
    "\n",
    "- **Put Operation**: The `put` method inserts a new key-value pair into the cache. If the key is already present, it updates the value and moves the node to the front of the list. If the cache is at capacity, it removes the node at the tail (least recently used) before adding the new node.\n",
    "\n",
    "### 7. **Runtime Complexity**\n",
    "\n",
    "- **Get Operation**: O(1) – The get operation involves a dictionary lookup and a linked list update, both of which are O(1) operations.\n",
    "- **Put Operation**: O(1) – Similarly, the put operation involves a dictionary update and potentially a linked list update, both of which are O(1) operations.\n",
    "\n",
    "### 8. **Why Use LRU Caching?**\n",
    "\n",
    "LRU caching is particularly useful in scenarios where we need to keep track of frequently accessed data that has a tendency to change over time, such as:\n",
    "\n",
    "- Caching database query results.\n",
    "- Caching HTTP responses in a web server.\n",
    "- Managing the memory of applications with limited resources, ensuring that the most useful data stays in the cache.\n",
    "\n",
    "### 9. **Conclusion**\n",
    "\n",
    "The LRU cache is an excellent example of combining different data structures (Maps and Linked Lists) to achieve an efficient and practical solution to a common problem in software engineering. Understanding how to implement and optimize such caches is a valuable skill, especially in performance-critical applications.\n",
    "\n",
    "If you have more questions or want to dive deeper into any specific aspect, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
